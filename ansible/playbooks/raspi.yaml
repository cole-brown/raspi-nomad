---

# Host patterns: https://docs.ansible.com/ansible/latest/user_guide/intro_patterns.html
# `&` is for an intersection of groups/hosts.
- hosts: dc_2021_home:&home_2019_raspi4

  # # ============================================================================
  # # Constants
  # # ============================================================================
  # vars:
  #   foo: "bar"

  # # ============================================================================
  # # Execute the roles.
  # # ============================================================================
  # roles:
  #   - foo.bar
  #   - foo.qux
  #   - narf

  # ============================================================================
  # Tasks:
  # ============================================================================
  tasks:
    # # --------------------------------------------------------------------------
    # # Debug
    # # --------------------------------------------------------------------------
    #
    # - name: Print | Facts
    #   ansible.builtin.debug:
    #     var: ansible_facts
    #
    # - name: Print | Vars | Host
    #   ansible.builtin.debug:
    #     var: hostvars["home_2019_raspi4"]
    #
    # - name: Print | Vars | Group
    #   ansible.builtin.debug:
    #     var: groups


    # --------------------------------------------------------------------------
    # [Set-Up] Gather Facts
    # --------------------------------------------------------------------------

    - name: Git Root | Gather Fact
      delegate_to: localhost
      command:
        cmd: "git rev-parse --show-toplevel"
      register: cmd_git_root
      changed_when: false

    - name: Git Root | Set Fact
      set_fact:
        path_git_root: "{{ cmd_git_root.stdout }}"


    # --------------------------------------------------------------------------
    # [Install] Prerequisites
    # --------------------------------------------------------------------------

    - name: Apt | Install | ca-certificates
      apt:
        name: ca-certificates
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.docker

    - name: Apt | Install | gnupg
      apt:
        name: gnupg
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.docker

    - name: Apt | Install | curl
      apt:
        name: curl
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.docker

    - name: Apt | Install | lsb-release
      apt:
        name: lsb-release
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.docker


    # --------------------------------------------------------------------------
    # [Install] Apt Repos
    # --------------------------------------------------------------------------
    # Add all extra apt repos before we run apt update/upgarde.

    - name: Apt | Docker Repo
      include_tasks: "apt.add-repo.yaml"
      vars:
        repo_item:
          name_display: Docker
          repo:
            # For repos added like: `deb [{{ architecture }} signed-by={{ dearmored }}] URL {{ distribution_release }} BRANCH`
            url: https://download.docker.com/linux/ubuntu
            branch: stable
          key:
            url: https://download.docker.com/linux/ubuntu/gpg
            basename: docker # docker.asc, docker.gpg
            checksum:
              # curl -fsSL https://download.docker.com/linux/ubuntu/gpg > /tmp/docker.asc
              # gpg --dearmor < /tmp/docker.asc | sha1sum
              # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/copy_module.html
              # `ansible.builtin.copy` uses `sha1` for checksumming.
              dearmored: "7c67919f823e005af75293a0edac7d0799252213"
      when: run.apt_repo

    - name: Apt | HashiCorp Repo
      include_tasks: "apt.add-repo.yaml"
      vars:
        repo_item:
          name_display: HashiCorp
          repo:
            # For repos added like: `deb [{{ architecture }}] URL {{ distribution_release }} BRANCH`
            url: https://apt.releases.hashicorp.com/
            branch: main
          key:
            url: https://apt.releases.hashicorp.com/gpg
            basename: hashicorp # hashicorp.asc, hashicorp.gpg
            checksum:
              # gpg --dearmor < /path/to/repo/ansible/gpg/hashicorp.asc | sha1sum
              # https://docs.ansible.com/ansible/latest/collections/ansible/builtin/copy_module.html
              # `ansible.builtin.copy` uses `sha1` for checksumming.
              dearmored: "dcece7980f1a183b1d59b26354247a29057786e0"
      when: run.apt_repo


    # --------------------------------------------------------------------------
    # [Install] Apt Prep.
    # --------------------------------------------------------------------------
    # Update the apt cache and upgrade existing packages.

    - name: Apt | Update Cache
      apt:
        update_cache: true
        cache_valid_time: 14400 # seconds == 4 hour
      # Don't care if this says the cache was old - only care if that causes
      # other steps to do something. So the status of this isn't as important -
      # those other steps will say if they changed.
      changed_when: false

    - name: Apt | Upgrade
      apt:
        upgrade: yes
      when: run.apt_upgrade


    # --------------------------------------------------------------------------
    # Docker
    # --------------------------------------------------------------------------

    # ------------------------------
    # Install
    # ------------------------------
    - name: Apt | Install | Docker Container Engine
      apt:
        name: docker-ce
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.docker

    - name: Apt | Install | Docker CE CLI
      apt:
        name: docker-ce-cli
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.docker

    - name: Apt | Install | containerd.io
      apt:
        name: containerd.io
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.docker

    - name: Apt | Install | Docker Compose Plugin
      apt:
        name: docker-compose-plugin
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.docker

    # Required for `community.docker.docker_network` Ansible module.
    # https://docs.ansible.com/ansible/latest/collections/community/docker/docker_network_module.html
    - name: PIP | Install | Docker SDK for Python
      pip:
        name: docker
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.docker

    # Required for `community.general.nomad_job` Ansible module.
    # https://docs.ansible.com/ansible/latest/collections/community/general/nomad_job_module.html
    - name: PIP | Install | Nomad SDK for Python
      pip:
        name: python-nomad
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.nomad

    # ------------------------------
    # Config
    # ------------------------------
    - name: Docker | Network | Create
      docker_network:
        name: raspi_vnet
        driver: macvlan
        driver_options:
          parent: eth0
        ipam_config:
          - subnet: 192.168.50.0/24
            iprange: 192.168.50.0/28
            gateway: 192.168.50.1
        state: present
      when: run.config.docker


    # --------------------------------------------------------------------------
    # HashiCorp Stuff: Nomad
    # --------------------------------------------------------------------------

    # ------------------------------
    # Install
    # ------------------------------
    - name: Apt | Install | HashiCorp Nomad
      apt:
        name: nomad
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.nomad

    # ------------------------------
    # Configure
    # ------------------------------
    - name: Nomad | Root Directory | Create
      file:
        state: directory
        path: "/srv/nomad"
        owner: "nomad"
        group: "nomad"
        recurse: no
      when: run.config.nomad

    #  If you do not run Nomad as root, make sure you add the Nomad user to the
    #  Docker group so Nomad can communicate with the Docker daemon.
    #    - https://www.nomadproject.io/docs/drivers/docker#client-requirements
    #  Just add it to the Docker group regardless?
    - name: Nomad | User Groups | Docker
      user:
        name: nomad
        groups: docker
        append: yes
      when: run.config.nomad

    - name: Nomad | Config Files | /etc/nomad.d
      copy:
        src: "{{ path_git_root }}/os/etc/nomad.d"
        dest: "/etc/"
        owner: "nomad"
        group: "nomad"
        mode: "u=rw,go=r"
        directory_mode: "u=rwx,go=rx"
      register: config_file_nomad
      when: run.config.nomad
      # notify:
      #   - restart-service-nomad

    - name: Nomad | Config Files | /etc/system.d/service/
      copy:
        src: "{{ path_git_root }}/os/etc/systemd/system/nomad.service"
        dest: "/etc/systemd/system/nomad.service"
        owner: "root"
        group: "root"
        mode: "u=rw,go=r"
        directory_mode: "u=rwx,go=rx"
      register: systemd_file_nomad
      when: run.config.nomad
      # notify:
      #   - restart-service-nomad


    # --------------------------------------------------------------------------
    # HashiCorp Stuff: Consul
    # --------------------------------------------------------------------------

    # ------------------------------
    # Install
    # ------------------------------
    - name: Apt | Install | HashiCorp Consul
      apt:
        name: consul
        state: present # Just need something installed.
        # state: latest # Make sure latest version is installed.
      when: run.install.consul

    # ------------------------------
    # Configure
    # ------------------------------
    - name: Consul | Config Files | /etc/consul.d
      copy:
        src: "{{ path_git_root }}/os/etc/consul.d"
        dest: "/etc/"
        owner: "consul"
        group: "consul"
        mode: "u=rw,go=r"
        directory_mode: "u=rwx,go=rx"
      register: config_file_consul
      when: run.config.consul
      # notify:
      #   - restart-service-consul

    - name: Consul | Config Files | /etc/system.d/service/
      copy:
        src: "{{ path_git_root }}/os/etc/systemd/system/consul.service"
        dest: "/etc/systemd/system/consul.service"
        owner: "root"
        group: "root"
        mode: "u=rw,go=r"
        directory_mode: "u=rwx,go=rx"
      register: systemd_file_consul
      when: run.config.consul
      # notify:
      #   - restart-service-consul


    # --------------------------------------------------------------------------
    # Network File Shares
    # --------------------------------------------------------------------------

    # ------------------------------
    # Install
    # ------------------------------
    - name: Apt | Install | CIFS Utils (for NFS)
      apt:
        name: cifs-utils
        state: present
      when: run.install.cifs

    # ------------------------------
    # Mount
    # ------------------------------
    - name: NFS | Root Directory | Create
      file:
        state: directory
        path: "/mnt/nfs"
        owner: "{{ os.user }}"
        group: "{{ os.group }}"
        recurse: no
      when: run.config.cifs

    - name: NFS | Mount Directory
      include_tasks: "nfs.add-mount.yaml"
      with_items: "{{ nfs.mounts }}"
      loop_control:
        loop_var: mount_item
      when: run.config.cifs


    # --------------------------------------------------------------------------
    # Nomad Volume Directories
    # --------------------------------------------------------------------------

    - name: Nomad | Host Volumes
      include_tasks: "nomad.host-volume.yaml"
      with_items: "{{ nomad.host_volumes }}"
      loop_control:
        loop_var: host_volume
      when: run.config.nomad


    # --------------------------------------------------------------------------
    # Nomad & Consul: Final Configuration
    # --------------------------------------------------------------------------
    # Nomad's service file has Consul as a `Wants` & `After` directive, so do
    # Consul first.

    # ------------------------------
    # System.D Daemon
    # ------------------------------
    - name: Services | Reload `system.d` Daemon for service file update
      systemd:
        daemon_reload: yes
      when: (systemd_file_consul.changed and run.config.consul) or (systemd_file_nomad.changed and run.config.nomad)

    # ------------------------------
    # Consul Final Config
    # ------------------------------
    - name: Consul | Service | Enable
      service:
        name: consul
        # Does nothing if already enabled & running.
        enabled: true
        state: started
      register: systemd_service_enabled_consul
      when: run.config.consul

    - name: Consul | Service | Restart
      service:
        name: consul
        state: restarted
      when:
        - systemd_file_consul.changed or config_file_consul.changed
        - not systemd_service_enabled_consul.changed
        - run.config.consul

    # - name: Consul | Web UI | Check
    #   uri:
    #     # NOTE: `8500` is the default port for Consul's web UI & `dc-2021-home` is our datacenter name in Consul/Nomad.
    #     url: "http://{{ ansible_host }}:8500/ui/dc-2021-home/services"
    #   when: run.config.consul

    - name: Consul | Service | Check Web UI
      wait_for:
        host: 127.0.0.1
        # NOTE: `8500` is the default port for Consul's web UI.
        port: 8500
      when: run.config.consul

    # ------------------------------
    # Nomad Final Config
    # ------------------------------
    - name: Nomad | Service | Enable
      service:
        name: nomad
        # Does nothing if already enabled & running.
        enabled: true
        state: started
      register: systemd_service_enabled_nomad
      when: run.config.nomad

    - name: Nomad | Service | Restart
      service:
        name: nomad
        state: restarted
      when:
        - systemd_file_nomad.changed or config_file_nomad.changed
        - not systemd_service_enabled_nomad.changed
        - run.config.nomad

    # - name: Nomad | Web UI | Check
    #   uri:
    #     # NOTE: `4646` is the default port for Nomad's web UI.
    #     url: "http://{{ ansible_host }}:4646/ui/jobs"
    #   when: run.config.nomad

    - name: Nomad | Service | Check Web UI
      wait_for:
        host: 127.0.0.1
        # NOTE: `4646` is the default port for Nomad's web UI.
        port: 4646
      when: run.config.nomad


    # --------------------------------------------------------------------------
    # Nomad Job: Pi-Hole
    # --------------------------------------------------------------------------

    - name: Nomad Job | Pi-Hole | Run Job
      community.general.nomad_job:
        host: localhost # This is run on the remote host `home_2019_raspi4`, so "localhost" == Ansible host == "the target"?
        content: "{{ lookup('ansible.builtin.file', '{{ path_git_root }}/nomad/jobs/pihole.nomad') }}"
        state: present
        use_ssl: no
      when: run.nomad.pihole
      register: nomad_job_pihole

    # Wait for it to be running... Can't get into the docker network from the
    # Raspberry Pi, so delegate to here.
    - name: Nomad Job | Pi-Hole | Check Web UI
      wait_for:
        host: 192.168.50.2
        port: 80
      delegate_to: localhost
      when:
        - run.nomad.pihole
        - nomad_job_pihole.changed

    # What is its Docker container called?
    - name: Nomad Job | Pi-Hole | Find Docker Container
      # Ansible and Docker both use Jinja2, so... Need to escape the Jinja2
      # double curlies with Jinja2 double curlies so that Ansible doesn't eat
      # the double curlies before Docker gets the double curlies.
      command: docker ps --filter ancestor='pihole/pihole' --format "{{ '{{' }} .Names {{ '}}' }}"
      when: run.nomad.pihole
      register: docker_container_pihole
      changed_when: false

    # Save whitelists before & diff after to check for changes.
    - name: Nomad Job | Pi-Hole | DNS | Get Whitelisted Domains (Before)
      community.docker.docker_container_exec:
        container: "{{ docker_container_pihole.stdout }}"
        argv:
          - pihole
          - whitelist
          - --list
      when:
        - run.nomad.pihole
        - docker_container_pihole.stdout
      register: pihole_whitelisted_domains_pre
      changed_when: false

    - name: Nomad Job | Pi-Hole | DNS | Get Whitelisted Regexes (Before)
      community.docker.docker_container_exec:
        container: "{{ docker_container_pihole.stdout }}"
        argv:
          - pihole
          - --white-regex
          - --list
      when:
        - run.nomad.pihole
        - docker_container_pihole.stdout
      register: pihole_whitelisted_regexes_pre
      changed_when: false

    # And now we should be able to run commands on the container whose name is
    # in `docker_container_pihole.stdout`.
    - name: Nomad Job | Pi-Hole | DNS | Whitelist Domains
      community.docker.docker_container_exec:
        container: "{{ docker_container_pihole.stdout }}"
        # Use `command` instead of `argv` so that space-separated domain list
        # becomes separate args properly.
        command: "pihole whitelist --noreload {{ whitelist_domains }}"
      with_items: "{{ pihole.whitelist.domains }}"
      loop_control:
        loop_var: whitelist_domains
      when:
        - run.nomad.pihole
        - docker_container_pihole.stdout
      register: pihole_whitelist_domains
      changed_when: false

    - name: Nomad Job | Pi-Hole | DNS | Whitelist Regexes
      community.docker.docker_container_exec:
        container: "{{ docker_container_pihole.stdout }}"
        argv:
          - pihole
          - --white-regex
          - --noreload
          - "{{ whitelist_regex }}"
      with_items: "{{ pihole.whitelist.regexes }}"
      loop_control:
        loop_var: whitelist_regex
      when:
        - run.nomad.pihole
        - docker_container_pihole.stdout
      register: pihole_whitelist_domains
      changed_when: false

    # Save whitelists before & diff after to check for changes.
    - name: Nomad Job | Pi-Hole | DNS | Get Whitelisted Domains (After)
      community.docker.docker_container_exec:
        container: "{{ docker_container_pihole.stdout }}"
        argv:
          - pihole
          - whitelist
          - --list
      when:
        - run.nomad.pihole
        - docker_container_pihole.stdout
      register: pihole_whitelisted_domains_post
      changed_when: false
      # TODO: Does this show all the whitelisted domains? I only see 45, which seems like too few...

    - name: Nomad Job | Pi-Hole | DNS | Get Whitelisted Regexes (After)
      community.docker.docker_container_exec:
        container: "{{ docker_container_pihole.stdout }}"
        argv:
          - pihole
          - --white-regex
          - --list
      when:
        - run.nomad.pihole
        - docker_container_pihole.stdout
      register: pihole_whitelisted_regexes_post
      changed_when: false

    # Reload the Pi-Hole DNS Server now that we're done with the changes, if the
    # whitelisting actually changed anything...
    - name: Nomad Job | Pi-Hole | DNS | Whitelists Changed?
      set_fact:
        pihole_whitelist_domains_changed: "{{ pihole_whitelisted_domains_pre != pihole_whitelisted_domains_post }}"
        pihole_whitelist_regexes_changed: "{{ pihole_whitelisted_regexes_pre != pihole_whitelisted_regexes_post }}"
      when:
        - run.nomad.pihole
        - docker_container_pihole.stdout
      # Try to make this step's status reflect its name?
      changed_when: pihole_whitelist_domains_changed or pihole_whitelist_regexes_changed

    - name: Nomad Job | Pi-Hole | DNS | Reload DNS Lists
      community.docker.docker_container_exec:
        container: "{{ docker_container_pihole.stdout }}"
        argv:
          - pihole
          - restartdns
          # Reload the DNS lists without flushing the cache or restarting the DNS server.
          - reload-lists
      when:
        - run.nomad.pihole
        - pihole_whitelist_domains_changed or pihole_whitelist_regexes_changed

    # To check your machine's DNS Server:
    #   ```
    #   systemd-resolve --status
    #   ```
    # But it's difficult to use that in the playbook since it won't immediately
    # change...
    # - name: Nomad Job | Pi-Hole | Using Pi-Hole for DNS?
    #   command:
    #     cmd: "systemd-resolve --status | grep 'DNS Server'"

    - name: Nomad Job | Pi-Hole | Backup | Make Filename
      set_fact:
        filename_pihole_backup: "pihole-backup.{{ ansible_date_time.iso8601_basic_short }}.tar.gz"
        path_docker_pihole_backup: "/var/local/pihole/backups"
        path_host_pihole_backup: "/srv/nomad/pihole/backups"
      when: run.nomad.pihole
      changed_when: false

    - name: Nomad Job | Pi-Hole | Backup | Ensure '{{ path_host_pihole_backup }}' Exists
      file:
        state: directory
        path: "{{ path_host_pihole_backup }}"
        owner: root
        group: root
        recurse: no
      when: run.nomad.pihole

    - name: Nomad Job | Pi-Hole | Backup | Make
      community.docker.docker_container_exec:
        container: "{{ docker_container_pihole.stdout }}"
        argv:
          - pihole
          - admin
          - teleporter
          - "{{ path_docker_pihole_backup }}/{{ filename_pihole_backup }}"
      when: run.nomad.pihole

    - name: Nomad Job | Pi-Hole | Backup | Fetch
      fetch:
        src: "{{ path_host_pihole_backup }}/{{ filename_pihole_backup }}"
        # Saves `src` file under "Ansible inventory host name" subdir of `dest`.
        dest: "{{ path_git_root }}/apps/pihole/backups"
      when: run.nomad.pihole


    # --------------------------------------------------------------------------
    # Nomad Job: Jackett
    # --------------------------------------------------------------------------

    - name: Nomad Job | Jackett | Run Job
      community.general.nomad_job:
        host: localhost # This is run on the remote host `home_2019_raspi4`, so "localhost" == Ansible host == "the target"?
        content: "{{ lookup('ansible.builtin.file', '{{ path_git_root }}/nomad/jobs/jackett.nomad') }}"
        state: present
        use_ssl: no
      when: run.nomad.jackett
      register: nomad_job_jackett

    # Wait for it to be running... Can't get into the docker network from the
    # Raspberry Pi, so delegate to here.
    - name: Nomad Job | Jackett | Check Web UI
      wait_for:
        host: 192.168.50.6
        port: 9117
      delegate_to: localhost
      when:
        - run.nomad.jackett
        - nomad_job_jackett.changed

    - name: Nomad Job | Jackett | Backup | Make Filename
      set_fact:
        filename_jackett_backup: "jackett-backup.{{ ansible_date_time.iso8601_basic_short }}.tar.gz"
        path_host_jackett_backup_dest: "/srv/nomad/jackett/backups"
        path_host_jackett_backup_src: "/srv/nomad/jackett/config/Jackett"
      when: run.nomad.jackett
      changed_when: false

    - name: Nomad Job | Jackett | Backup | Ensure '{{ path_host_jackett_backup_dest }}' Exists
      file:
        state: directory
        path: "{{ path_host_jackett_backup_dest }}"
        owner: root
        group: root
        recurse: no
      when: run.nomad.jackett

    - name: Nomad Job | Jackett | Backup | Make
      community.general.archive:
        path: "{{ path_host_jackett_backup_src }}"
        exclusion_patterns:
          - "*log.txt"
        dest: "{{ path_host_jackett_backup_dest }}/{{ filename_jackett_backup }}"
      when: run.nomad.jackett

    - name: Nomad Job | Jackett | Backup | Fetch
      fetch:
        src: "{{ path_host_jackett_backup_dest }}/{{ filename_jackett_backup }}"
        # Saves `src` file under "Ansible inventory host name" subdir of `dest`.
        dest: "{{ path_git_root }}/apps/jackett/backups"
      when: run.nomad.jackett


    # --------------------------------------------------------------------------
    # [Clean-Up]
    # --------------------------------------------------------------------------

    - name: Apt | Auto-Clean
      apt:
        autoclean: yes
      when: run.apt_clean_up

    - name: Apt | Auto-Remove
      apt:
        autoremove: yes
      when: run.apt_clean_up


  # ============================================================================
  # Handlers
  # ============================================================================
  #
  # NOTE: Currently not using these as I need to have the services running their
  # latest config/etc ASAP. Maybe switching to roles and having Nomad/Consul
  # installed in separate roles than the "install Nomad services" parts would
  # make these handlers useful?
  #
  # handlers:
  #   # Restart `nomad` service to pick up new config settings.
  #   - name: restart-service-nomad
  #     service:
  #       name: nomad
  #       state: restarted
  #   # Restart `consul` service to pick up new config settings.
  #   - name: restart-service-consul
  #     service:
  #       name: consul
  #       state: restarted
